{"cells":[{"cell_type":"markdown","metadata":{"id":"uO5uHlDpOHIG"},"source":["# Importing The Data from The Material Project Database using their API"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":169332,"status":"ok","timestamp":1733927926773,"user":{"displayName":"Muhamad Kussai Alawad","userId":"07092718358044543588"},"user_tz":-60},"id":"01soD4bBFXqv","outputId":"2a9e05e7-7a5b-4fed-bf85-417df9e511a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"]},{"name":"stderr","output_type":"stream","text":["Fetching empirical data: 100%|██████████| 49/49 [00:57<00:00,  1.17s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Saved 48884 records to empirical_data_f2_p1.json\n"]},{"name":"stderr","output_type":"stream","text":["Fetching theoretical data: 100%|██████████| 105/105 [01:45<00:00,  1.00s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Saved 104351 records to theoretical_data_f2_p1.json\n"]}],"source":["# Install required libraries\n","!pip install tqdm\n","\n","import requests\n","import json\n","from tqdm import tqdm\n","from google.colab import files\n","\n","# Your Materials Project API key\n","API_KEY = 'UUNzWnc9qbnzxZxJ3UwzXN4On33FSMDF'\n","\n","# Base URL for the Materials Project API\n","BASE_URL = 'https://api.materialsproject.org/materials/summary/'\n","\n","# Headers with the API key\n","headers = {\n","    'X-API-KEY': API_KEY\n","}\n","\n","def fetch_data(theoretical, max_pages, output_filename):\n","    \"\"\"\n","    Fetch data from the Materials Project API.\n","\n","    Parameters:\n","    - theoretical (bool): Whether to fetch theoretical data.\n","    - max_pages (int): Number of pages to fetch.\n","    - output_filename (str): Filename to save the JSON data.\n","    \"\"\"\n","    all_data = []\n","    for page in tqdm(range(1, max_pages + 1), desc=f'Fetching {\"theoretical\" if theoretical else \"empirical\"} data'):\n","        params = {\n","            'theoretical': str(theoretical).lower(),\n","            'deprecated': 'false',\n","            '_page': page,\n","            '_per_page': 1000,\n","            '_skip': 0,\n","            '_limit': 1000,\n","            '_fields': 'formula_pretty,composition_reduced,density,ordering',\n","            '_all_fields': 'false',\n","            'license': 'BY-C'\n","        }\n","\n","        try:\n","            response = requests.get(BASE_URL, headers=headers, params=params)\n","            response.raise_for_status()  # Raise an error for bad status codes\n","            json_response = response.json()\n","\n","            # Ensure that 'data' is in the response\n","            if 'data' not in json_response:\n","                print(f\"No 'data' field found in response on page {page}. Response content:\")\n","                print(json_response)\n","                break\n","\n","            data = json_response['data']\n","\n","            if not data:\n","                print(f\"No data found on page {page}. Ending fetch.\")\n","                break\n","\n","            all_data.extend(data)\n","        except requests.exceptions.HTTPError as http_err:\n","            print(f\"HTTP error occurred on page {page}: {http_err}\")\n","            break\n","        except Exception as err:\n","            print(f\"An error occurred on page {page}: {err}\")\n","            break\n","\n","    # Save the collected data to a JSON file\n","    with open(output_filename, 'w') as f:\n","        json.dump(all_data, f, indent=4)\n","    print(f\"Saved {len(all_data)} records to {output_filename}\")\n","\n","# Fetch empirical data (theoretical=false) up to page 49\n","fetch_data(theoretical=False, max_pages=49, output_filename='empirical_data_f2_p1.json')\n","\n","# Fetch theoretical data (theoretical=true) up to page 105\n","fetch_data(theoretical=True, max_pages=105, output_filename='theoretical_data_f2_p1.json')\n","\n","# Optional: Download the JSON files to your local machine\n","# Commented out to prevent automatic downloads during execution\n","# files.download('empirical_data_f2_p1.json')\n","# files.download('theoretical_data_f2_p1.json')\n"]},{"cell_type":"markdown","metadata":{"id":"EW39XcIPhXKW"},"source":["# Erasing unlabeled data or data labeled unknown"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3320,"status":"ok","timestamp":1733941873520,"user":{"displayName":"Muhamad Kussai Alawad","userId":"07092718358044543588"},"user_tz":-60},"id":"OQt30Eqr355N","outputId":"3bab4e9e-4b13-4d09-8a2b-2d57ab714800"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"]}],"source":["# Install required libraries (if not already installed)\n","!pip install tqdm\n","\n","import json\n","from tqdm import tqdm\n","from google.colab import files\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcGxt5JS4L1C"},"outputs":[],"source":["def process_json_file(input_filename, labeled_output_filename, unlabeled_list):\n","    \"\"\"\n","    Processes a JSON file to separate labeled and unlabeled data points based on the 'ordering' field.\n","\n","    Parameters:\n","    - input_filename (str): Path to the input JSON file.\n","    - labeled_output_filename (str): Path to save the labeled data.\n","    - unlabeled_list (list): A list to append unlabeled data points.\n","\n","    Returns:\n","    - int: Number of data points processed.\n","    - int: Number of labeled data points.\n","    - int: Number of unlabeled data points.\n","    \"\"\"\n","    with open(input_filename, 'r') as f:\n","        data = json.load(f)\n","\n","    labeled_data = []\n","    unlabeled_data = []\n","\n","    for entry in tqdm(data, desc=f'Processing {input_filename}'):\n","        ordering = entry.get('ordering', None)\n","\n","        # Check if 'ordering' is \"unknown\", \"None\", or None (null)\n","        if ordering is None or ordering == \"unknown\" or ordering == \"None\":\n","            # Set 'ordering' to \"unknown\"\n","            entry['ordering'] = \"unknown\"\n","            unlabeled_data.append(entry)\n","        else:\n","            labeled_data.append(entry)\n","\n","    # Append unlabeled data to the main unlabeled_list\n","    unlabeled_list.extend(unlabeled_data)\n","\n","    # Save the labeled data to the labeled_output_filename\n","    with open(labeled_output_filename, 'w') as f:\n","        json.dump(labeled_data, f, indent=4)\n","\n","    print(f\"Processed {len(data)} records from {input_filename}:\")\n","    print(f\" - Labeled records: {len(labeled_data)}\")\n","    print(f\" - Unlabeled records: {len(unlabeled_data)}\\n\")\n","\n","    return len(data), len(labeled_data), len(unlabeled_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkt73SHX4REy"},"outputs":[],"source":["# Initialize an empty list to hold all unlabeled data\n","unlabeled_f2_p1 = []\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":309},"executionInfo":{"elapsed":419,"status":"error","timestamp":1733942027848,"user":{"displayName":"Muhamad Kussai Alawad","userId":"07092718358044543588"},"user_tz":-60},"id":"3XX1iKd14RRs","outputId":"718c2b91-648f-4de1-c6e9-49e4196c6ed7"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'empirical_data_f2_p1.json'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-4468f7ee9f19>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Process the empirical data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprocess_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempirical_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mempirical_labeled_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_f2_p1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-48-546a30f25bad>\u001b[0m in \u001b[0;36mprocess_json_file\u001b[0;34m(input_filename, labeled_output_filename, unlabeled_list)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0munlabeled\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'empirical_data_f2_p1.json'"]}],"source":["# Define file names\n","empirical_input = 'empirical_data_f2_p1.json'\n","empirical_labeled_output = 'labeled_empirical_data_f2_p1.json'\n","\n","# Process the empirical data\n","process_json_file(empirical_input, empirical_labeled_output, unlabeled_f2_p1)\n"]},{"cell_type":"markdown","metadata":{"id":"ziZc-7NZOXcT"},"source":["# Dividing the Data into Training, Training_Dev, Dev, and Testing datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":315572,"status":"ok","timestamp":1733942389537,"user":{"displayName":"Muhamad Kussai Alawad","userId":"07092718358044543588"},"user_tz":-60},"id":"bJ49qe3U4QVC","outputId":"227bad4a-b5a1-4ca3-c4d6-aba6094e21cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✅ Target directory already exists: /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data\n","✅ Loaded 48649 records from /content/drive/MyDrive/Colab Notebooks/Datasets/RawData/labeled_empirical_data_f2_p1.json\n","\n","🔄 Splitting empirical data into Dev, Test, and Training sets...\n","✅ Saved 7500 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data/dev_f2_p1.json\n","✅ Saved 7500 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data/test_f2_p1.json\n","✅ Saved 33649 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data/training_labeled_empirical_data_f2_p1.json\n","✅ Loaded 103533 records from /content/drive/MyDrive/Colab Notebooks/Datasets/RawData/labeled_theoretical_data_f2_p1.json\n","\n","🔄 Combining remaining empirical data with theoretical data...\n","✅ Saved 137182 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data/labeled_empirical_theoretical_data_f2_p1.json\n","\n","🔄 Splitting combined data into Training Dev and Training sets...\n","✅ Saved 7500 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data/training_dev_f2_p1.json\n","✅ Saved 129670 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data/training_f2_p1.json\n","\n","🎉 Data segmentation and organization complete!\n","\n","📊 Verification of dataset sizes:\n","Dev set: 7500 records\n","Test set: 7500 records\n","Training (Empirical) set: 33649 records\n","Combined Empirical & Theoretical set: 137182 records\n","Training Dev set: 7500 records\n","Training set: 129670 records\n"]}],"source":["# Install required libraries (if not already installed)\n","!pip install tqdm\n","\n","# Import necessary libraries\n","import os\n","import json\n","import random\n","import shutil\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define source and target directories\n","SOURCE_DIR = '/content/drive/MyDrive/Colab Notebooks/Datasets/RawData'\n","TARGET_DIR = '/content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data'\n","\n","# Create the target directory if it doesn't exist\n","if not os.path.exists(TARGET_DIR):\n","    os.makedirs(TARGET_DIR)\n","    print(f\"✅ Created target directory: {TARGET_DIR}\")\n","else:\n","    print(f\"✅ Target directory already exists: {TARGET_DIR}\")\n","\n","# Define filenames\n","# Source files\n","LABELED_EMPIRICAL = 'labeled_empirical_data_f2_p1.json'\n","LABELED_THEORETICAL = 'labeled_theoretical_data_f2_p1.json'\n","\n","# Intermediate files\n","TRAINING_LABELED_EMPIRICAL = 'training_labeled_empirical_data_f2_p1.json'\n","LABELED_EMPIRICAL_THEORETICAL = 'labeled_empirical_theoretical_data_f2_p1.json'\n","\n","# Output files\n","DEV_FILE = 'dev_f2_p1.json'\n","TEST_FILE = 'test_f2_p1.json'\n","TRAINING_DEV_FILE = 'training_dev_f2_p1.json'\n","TRAINING_FILE = 'training_f2_p1.json'\n","\n","# Full paths for source files\n","empirical_path = os.path.join(SOURCE_DIR, LABELED_EMPIRICAL)\n","theoretical_path = os.path.join(SOURCE_DIR, LABELED_THEORETICAL)\n","\n","# Full paths for intermediate and output files in target directory\n","training_labeled_empirical_path = os.path.join(TARGET_DIR, TRAINING_LABELED_EMPIRICAL)\n","labeled_empirical_theoretical_path = os.path.join(TARGET_DIR, LABELED_EMPIRICAL_THEORETICAL)\n","\n","dev_path = os.path.join(TARGET_DIR, DEV_FILE)\n","test_path = os.path.join(TARGET_DIR, TEST_FILE)\n","training_dev_path = os.path.join(TARGET_DIR, TRAINING_DEV_FILE)\n","training_path = os.path.join(TARGET_DIR, TRAINING_FILE)\n","\n","# Function to load JSON data\n","def load_json(file_path):\n","    try:\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"✅ Loaded {len(data)} records from {file_path}\")\n","        return data\n","    except FileNotFoundError:\n","        print(f\"❌ File not found: {file_path}\")\n","        return []\n","    except json.JSONDecodeError:\n","        print(f\"❌ JSON decode error in file: {file_path}\")\n","        return []\n","    except Exception as e:\n","        print(f\"❌ Unexpected error loading {file_path}: {e}\")\n","        return []\n","\n","# Function to save JSON data\n","def save_json(data, file_path):\n","    try:\n","        with open(file_path, 'w') as f:\n","            json.dump(data, f, indent=4)\n","        print(f\"✅ Saved {len(data)} records to {file_path}\")\n","    except Exception as e:\n","        print(f\"❌ Error saving to {file_path}: {e}\")\n","\n","# Function to randomly sample data\n","def sample_data(data, sample_size):\n","    return random.sample(data, sample_size)\n","\n","# Function to split data into subsets\n","def split_data(data, dev_size, test_size):\n","    random.shuffle(data)\n","    dev_data = data[:dev_size]\n","    test_data = data[dev_size:dev_size + test_size]\n","    training_data = data[dev_size + test_size:]\n","    return dev_data, test_data, training_data\n","\n","# Function to combine and shuffle datasets\n","def combine_and_shuffle(data1, data2):\n","    combined = data1 + data2\n","    random.shuffle(combined)\n","    return combined\n","\n","# Seed for reproducibility\n","random.seed(42)\n","\n","# Step 1: Load labeled_empirical_data_f2_p1.json\n","empirical_data = load_json(empirical_path)\n","if not empirical_data:\n","    raise ValueError(\"Empirical data could not be loaded. Please check the file and try again.\")\n","\n","# Check if there are at least 15,000 records\n","if len(empirical_data) < 15000:\n","    raise ValueError(f\"Insufficient data in {LABELED_EMPIRICAL}. Required: 15,000, Available: {len(empirical_data)}\")\n","\n","# Step 2: Split empirical data into Dev, Test, and Training\n","print(\"\\n🔄 Splitting empirical data into Dev, Test, and Training sets...\")\n","dev_data, test_data, remaining_empirical = split_data(empirical_data, 7500, 7500)\n","\n","# Save Dev and Test sets\n","save_json(dev_data, dev_path)\n","save_json(test_data, test_path)\n","\n","# Save the remaining data to training_labeled_empirical_data_f2_p1.json\n","save_json(remaining_empirical, training_labeled_empirical_path)\n","\n","# Step 3: Load labeled_theoretical_data_f2_p1.json\n","theoretical_data = load_json(theoretical_path)\n","if not theoretical_data:\n","    raise ValueError(\"Theoretical data could not be loaded. Please check the file and try again.\")\n","\n","# Step 4: Combine remaining empirical data with theoretical data\n","print(\"\\n🔄 Combining remaining empirical data with theoretical data...\")\n","combined_empirical_theoretical = combine_and_shuffle(remaining_empirical, theoretical_data)\n","\n","# Save the combined data\n","save_json(combined_empirical_theoretical, labeled_empirical_theoretical_path)\n","\n","# Step 5: Split combined data into Training Dev and Training sets\n","print(\"\\n🔄 Splitting combined data into Training Dev and Training sets...\")\n","if len(combined_empirical_theoretical) < 7500:\n","    raise ValueError(f\"Insufficient combined data for Training Dev set. Required: 7,500, Available: {len(combined_empirical_theoretical)}\")\n","\n","training_dev_data = sample_data(combined_empirical_theoretical, 7500)\n","training_f2_p1_data = [record for record in combined_empirical_theoretical if record not in training_dev_data]\n","\n","# Save Training Dev and Training sets\n","save_json(training_dev_data, training_dev_path)\n","save_json(training_f2_p1_data, training_path)\n","\n","print(\"\\n🎉 Data segmentation and organization complete!\")\n","\n","# Optional: Verify the results\n","def verify_counts():\n","    print(\"\\n📊 Verification of dataset sizes:\")\n","    print(f\"Dev set: {len(dev_data)} records\")\n","    print(f\"Test set: {len(test_data)} records\")\n","    print(f\"Training (Empirical) set: {len(remaining_empirical)} records\")\n","    print(f\"Combined Empirical & Theoretical set: {len(combined_empirical_theoretical)} records\")\n","    print(f\"Training Dev set: {len(training_dev_data)} records\")\n","    print(f\"Training set: {len(training_f2_p1_data)} records\")\n","\n","verify_counts()\n"]},{"cell_type":"markdown","metadata":{"id":"Ao6iDacsX4vs"},"source":["# Using One_Hot encoding"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39125,"status":"ok","timestamp":1734022446997,"user":{"displayName":"Muhamad Kussai Alawad","userId":"07092718358044543588"},"user_tz":-60},"id":"sXaqXJXTI5wF","outputId":"f6d6c49c-9ab5-43a8-a170-c88aab0a3419"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✅ Target directory already exists: /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 2 Data\n","\n","🔄 Processing file: dev_f2_p1.json\n","✅ Loaded 7500 records from /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data/dev_f2_p1.json\n"]},{"output_type":"stream","name":"stderr","text":["Processing dev_f2_p1.json: 100%|██████████| 7500/7500 [00:00<00:00, 31228.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Saved 7500 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 2 Data/dev_f2_p2.json\n","\n","🔄 Processing file: test_f2_p1.json\n","✅ Loaded 7500 records from /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data/test_f2_p1.json\n"]},{"output_type":"stream","name":"stderr","text":["Processing test_f2_p1.json: 100%|██████████| 7500/7500 [00:00<00:00, 29523.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Saved 7500 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 2 Data/test_f2_p2.json\n","\n","🔄 Processing file: training_f2_p1.json\n","✅ Loaded 129670 records from /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data/training_f2_p1.json\n"]},{"output_type":"stream","name":"stderr","text":["Processing training_f2_p1.json: 100%|██████████| 129670/129670 [00:05<00:00, 23004.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Saved 129670 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 2 Data/training_f2_p2.json\n","\n","🔄 Processing file: training_dev_f2_p1.json\n","✅ Loaded 7500 records from /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data/training_dev_f2_p1.json\n"]},{"output_type":"stream","name":"stderr","text":["Processing training_dev_f2_p1.json: 100%|██████████| 7500/7500 [00:00<00:00, 22861.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Saved 7500 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 2 Data/training_dev_f2_p2.json\n","\n","🎉 One-Hot Encoding and Data Transformation Complete!\n","\n","🔍 Inspecting the newly created Phase 2 Data files:\n","📄 dev_f2_p2.json: 7500 records\n","📝 Sample record from dev_f2_p2.json:\n","{\n","    \"formula_pretty\": \"Cs2MnH12(SeO7)2\",\n","    \"composition_reduced\": {\n","        \"Cs\": 2.0,\n","        \"Mn\": 1.0,\n","        \"H\": 12.0,\n","        \"Se\": 2.0,\n","        \"O\": 14.0\n","    },\n","    \"composition_encoded\": [\n","        12.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        14.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        1.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        2.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        2.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0\n","    ],\n","    \"density\": 2.811677638615824,\n","    \"ordering\": \"FM\",\n","    \"ordering_encoded\": [\n","        1,\n","        0,\n","        0,\n","        0\n","    ]\n","}\n","📄 test_f2_p2.json: 7500 records\n","📝 Sample record from test_f2_p2.json:\n","{\n","    \"formula_pretty\": \"Nd2B5\",\n","    \"composition_reduced\": {\n","        \"Nd\": 2.0,\n","        \"B\": 5.0\n","    },\n","    \"composition_encoded\": [\n","        0,\n","        0,\n","        0,\n","        0,\n","        5.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        2.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0\n","    ],\n","    \"density\": 5.941578193097611,\n","    \"ordering\": \"NM\",\n","    \"ordering_encoded\": [\n","        0,\n","        1,\n","        0,\n","        0\n","    ]\n","}\n","📄 training_f2_p2.json: 129670 records\n","📝 Sample record from training_f2_p2.json:\n","{\n","    \"formula_pretty\": \"CaAl2(SiO3)4\",\n","    \"composition_reduced\": {\n","        \"Ca\": 1.0,\n","        \"Al\": 2.0,\n","        \"Si\": 4.0,\n","        \"O\": 12.0\n","    },\n","    \"composition_encoded\": [\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        12.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        2.0,\n","        4.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        1.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0\n","    ],\n","    \"density\": 3.1468685062970696,\n","    \"ordering\": \"NM\",\n","    \"ordering_encoded\": [\n","        0,\n","        1,\n","        0,\n","        0\n","    ]\n","}\n","📄 training_dev_f2_p2.json: 7500 records\n","📝 Sample record from training_dev_f2_p2.json:\n","{\n","    \"formula_pretty\": \"Ag3Ge5P6\",\n","    \"composition_reduced\": {\n","        \"Ag\": 3.0,\n","        \"Ge\": 5.0,\n","        \"P\": 6.0\n","    },\n","    \"composition_encoded\": [\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        6.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        5.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        3.0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0,\n","        0\n","    ],\n","    \"density\": 5.220634131745827,\n","    \"ordering\": \"NM\",\n","    \"ordering_encoded\": [\n","        0,\n","        1,\n","        0,\n","        0\n","    ]\n","}\n"]}],"source":["# Install required libraries\n","!pip install tqdm\n","\n","# Import necessary libraries\n","import os\n","import json\n","import random\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","element_symbols = [\n","    \"H\", \"He\",\n","    \"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"F\", \"Ne\",\n","    \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\", \"Cl\", \"Ar\",\n","    \"K\", \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\",\n","    \"Cu\", \"Zn\", \"Ga\", \"Ge\", \"As\", \"Se\", \"Br\", \"Kr\",\n","    \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\",\n","    \"Ag\", \"Cd\", \"In\", \"Sn\", \"Sb\", \"Te\", \"I\", \"Xe\",\n","    \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\",\n","    \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\", \"Yb\", \"Lu\",\n","    \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Hg\",\n","    \"Tl\", \"Pb\", \"Bi\", \"Po\", \"At\", \"Rn\",\n","    \"Fr\", \"Ra\", \"Ac\", \"Th\", \"Pa\", \"U\", \"Np\", \"Pu\",\n","    \"Am\", \"Cm\", \"Bk\", \"Cf\", \"Es\", \"Fm\", \"Md\", \"No\",\n","    \"Lr\", \"Rf\", \"Db\", \"Sg\", \"Bh\", \"Hs\", \"Mt\", \"Ds\",\n","    \"Rg\", \"Cn\", \"Nh\", \"Fl\", \"Mc\", \"Lv\", \"Ts\", \"Og\"\n","]\n","\n","\n","# Define ordering classes and their one-hot encoding\n","ordering_classes = [\"FM\", \"NM\", \"FiM\", \"AFM\"]\n","ordering_one_hot = {\n","    \"FM\": [1, 0, 0, 0],\n","    \"NM\": [0, 1, 0, 0],\n","    \"FiM\": [0, 0, 1, 0],\n","    \"AFM\": [0, 0, 0, 1]\n","}\n","\n","# Define source and target directories\n","SOURCE_DIR = '/content/drive/MyDrive/Colab Notebooks/Datasets/Phase 1 Data'\n","TARGET_DIR = '/content/drive/MyDrive/Colab Notebooks/Datasets/Phase 2 Data'\n","\n","# Create the target directory if it doesn't exist\n","if not os.path.exists(TARGET_DIR):\n","    os.makedirs(TARGET_DIR)\n","    print(f\"✅ Created target directory: {TARGET_DIR}\")\n","else:\n","    print(f\"✅ Target directory already exists: {TARGET_DIR}\")\n","\n","# Define the four source files\n","source_files = [\n","    'dev_f2_p1.json',\n","    'test_f2_p1.json',\n","    'training_f2_p1.json',\n","    'training_dev_f2_p1.json'\n","]\n","\n","# Define a function to load JSON data\n","def load_json(file_path):\n","    try:\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"✅ Loaded {len(data)} records from {file_path}\")\n","        return data\n","    except FileNotFoundError:\n","        print(f\"❌ File not found: {file_path}\")\n","        return []\n","    except json.JSONDecodeError:\n","        print(f\"❌ JSON decode error in file: {file_path}\")\n","        return []\n","    except Exception as e:\n","        print(f\"❌ Unexpected error loading {file_path}: {e}\")\n","        return []\n","\n","# Define a function to save JSON data\n","def save_json(data, file_path):\n","    try:\n","        with open(file_path, 'w') as f:\n","            json.dump(data, f, indent=4)\n","        print(f\"✅ Saved {len(data)} records to {file_path}\")\n","    except Exception as e:\n","        print(f\"❌ Error saving to {file_path}: {e}\")\n","\n","def encode_composition(composition_reduced):\n","    composition_encoded = []\n","    for element in element_symbols:\n","        count = composition_reduced.get(element, 0)\n","        # If count is not an integer or float, treat it as 0.\n","        # Removed the print warning to avoid console clutter.\n","        if not isinstance(count, (int, float)):\n","            count = 0\n","        composition_encoded.append(count)\n","\n","    # Assert that we have exactly 118 elements after encoding.\n","    assert len(composition_encoded) == 118, (\n","        f\"composition_encoded length is {len(composition_encoded)}, \"\n","        f\"expected 118. Please verify your element_symbols list and data processing.\"\n","    )\n","\n","    return composition_encoded\n","\n","# Define a function to encode ordering\n","def encode_ordering(ordering):\n","    encoded = ordering_one_hot.get(ordering, [0, 0, 0, 0])\n","    if encoded == [0, 0, 0, 0]:\n","        print(f\"⚠️ Unrecognized ordering class: {ordering}. Encoding as all zeros.\")\n","    return encoded\n","\n","# Process each source file\n","for source_file in source_files:\n","    source_path = os.path.join(SOURCE_DIR, source_file)\n","    # Create the target file name by replacing 'p1' with 'p2'\n","    target_file = source_file.replace('p1', 'p2')\n","    target_path = os.path.join(TARGET_DIR, target_file)\n","\n","    print(f\"\\n🔄 Processing file: {source_file}\")\n","\n","    # Load the data\n","    data = load_json(source_path)\n","    if not data:\n","        print(f\"❌ Skipping file due to loading issues: {source_file}\")\n","        continue\n","\n","    processed_data = []\n","\n","    # Process each record\n","    for record in tqdm(data, desc=f\"Processing {source_file}\"):\n","        # Extract required fields\n","        formula_pretty = record.get('formula_pretty', \"\")\n","        composition_reduced = record.get('composition_reduced', {})\n","        density = record.get('density', None)\n","        ordering = record.get('ordering', \"\")\n","\n","        # Encode composition\n","        composition_encoded = encode_composition(composition_reduced)\n","\n","        # Encode ordering\n","        ordering_encoded = encode_ordering(ordering)\n","\n","        # Reconstruct the record with the specified key order\n","        new_record = {\n","            \"formula_pretty\": formula_pretty,\n","            \"composition_reduced\": composition_reduced,\n","            \"composition_encoded\": composition_encoded,\n","            \"density\": density,\n","            \"ordering\": ordering,\n","            \"ordering_encoded\": ordering_encoded\n","        }\n","\n","        processed_data.append(new_record)\n","\n","    # Save the processed data to the target file\n","    save_json(processed_data, target_path)\n","\n","print(\"\\n🎉 One-Hot Encoding and Data Transformation Complete!\")\n","\n","# Optional: Verify the new files\n","def inspect_new_files():\n","    print(\"\\n🔍 Inspecting the newly created Phase 2 Data files:\")\n","    new_files = [\n","        'dev_f2_p2.json',\n","        'test_f2_p2.json',\n","        'training_f2_p2.json',\n","        'training_dev_f2_p2.json'\n","    ]\n","    for file in new_files:\n","        file_path = os.path.join(TARGET_DIR, file)\n","        if os.path.exists(file_path):\n","            with open(file_path, 'r') as f:\n","                data = json.load(f)\n","            print(f\"📄 {file}: {len(data)} records\")\n","            if len(data) > 0:\n","                sample = data[0]\n","                print(f\"📝 Sample record from {file}:\")\n","                print(json.dumps(sample, indent=4))\n","        else:\n","            print(f\"❌ File not found: {file_path}\")\n","\n","inspect_new_files()\n"]},{"cell_type":"markdown","metadata":{"id":"jK03WUplS7Sg"},"source":["# Turning the Data into Numpy Arrays"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"r0br9E2rU5sU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734022493934,"user_tz":-60,"elapsed":46943,"user":{"displayName":"Muhamad Kussai Alawad","userId":"07092718358044543588"}},"outputId":"2219c5cf-e4ca-4c65-bbf5-ac26ce780f72"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✅ Created directory: /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data\n","✅ Created subdirectory: /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/dictionaries\n","✅ Created subdirectory: /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/Data_Tensorflow\n","✅ Loaded 7500 records from /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 2 Data/dev_f2_p2.json\n"]},{"output_type":"stream","name":"stderr","text":["Processing dev_f2_p2.json: 100%|██████████| 7500/7500 [00:00<00:00, 83817.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Saved 7500 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/dictionaries/dev_f2_p2_dict.json\n","✅ Loaded 7500 records from /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 2 Data/test_f2_p2.json\n"]},{"output_type":"stream","name":"stderr","text":["Processing test_f2_p2.json: 100%|██████████| 7500/7500 [00:00<00:00, 136864.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Saved 7500 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/dictionaries/test_f2_p2_dict.json\n","✅ Loaded 129670 records from /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 2 Data/training_f2_p2.json\n"]},{"output_type":"stream","name":"stderr","text":["Processing training_f2_p2.json: 100%|██████████| 129670/129670 [00:01<00:00, 70012.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Saved 129670 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/dictionaries/training_f2_p2_dict.json\n","✅ Loaded 7500 records from /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 2 Data/training_dev_f2_p2.json\n"]},{"output_type":"stream","name":"stderr","text":["Processing training_dev_f2_p2.json: 100%|██████████| 7500/7500 [00:00<00:00, 265848.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Saved 7500 records to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/dictionaries/training_dev_f2_p2_dict.json\n","✅ Loaded 7500 records from /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/dictionaries/dev_f2_p2_dict.json\n","✅ Saved feature array to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/Data_Tensorflow/X_dev_f2_p2.npy\n","✅ Saved label array to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/Data_Tensorflow/Y_dev_f2_p2.npy\n","✅ Loaded 7500 records from /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/dictionaries/test_f2_p2_dict.json\n","✅ Saved feature array to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/Data_Tensorflow/X_test_f2_p2.npy\n","✅ Saved label array to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/Data_Tensorflow/Y_test_f2_p2.npy\n","✅ Loaded 129670 records from /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/dictionaries/training_f2_p2_dict.json\n","✅ Saved feature array to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/Data_Tensorflow/X_training_f2_p2.npy\n","✅ Saved label array to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/Data_Tensorflow/Y_training_f2_p2.npy\n","✅ Loaded 7500 records from /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/dictionaries/training_dev_f2_p2_dict.json\n","✅ Saved feature array to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/Data_Tensorflow/X_training_dev_f2_p2.npy\n","✅ Saved label array to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/Data_Tensorflow/Y_training_dev_f2_p2.npy\n","\n","🎉 All processing complete! The 'Phase 3 Data' folder now contains the 'dictionaries' and 'Data_Tensorflow' subfolders with the respective files.\n"]}],"source":["# Install required libraries\n","!pip install tqdm\n","\n","# Import necessary libraries\n","import os\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define source and target directories\n","PHASE_2_DIR = '/content/drive/MyDrive/Colab Notebooks/Datasets/Phase 2 Data'\n","PHASE_3_DIR = '/content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data'\n","DICTIONARIES_DIR = os.path.join(PHASE_3_DIR, 'dictionaries')\n","DATA_TENSORFLOW_DIR = os.path.join(PHASE_3_DIR, 'Data_Tensorflow')\n","\n","# Create Phase 3 Data directory if it doesn't exist\n","if not os.path.exists(PHASE_3_DIR):\n","    os.makedirs(PHASE_3_DIR)\n","    print(f\"✅ Created directory: {PHASE_3_DIR}\")\n","else:\n","    print(f\"✅ Directory already exists: {PHASE_3_DIR}\")\n","\n","# Create 'dictionaries' and 'Data_Tensorflow' subdirectories\n","for subdir in [DICTIONARIES_DIR, DATA_TENSORFLOW_DIR]:\n","    if not os.path.exists(subdir):\n","        os.makedirs(subdir)\n","        print(f\"✅ Created subdirectory: {subdir}\")\n","    else:\n","        print(f\"✅ Subdirectory already exists: {subdir}\")\n","\n","# Define the four source files\n","source_files = {\n","    'dev_f2_p2.json': 'dev_f2_p2_dict.json',\n","    'test_f2_p2.json': 'test_f2_p2_dict.json',\n","    'training_f2_p2.json': 'training_f2_p2_dict.json',\n","    'training_dev_f2_p2.json': 'training_dev_f2_p2_dict.json'\n","}\n","\n","# Define ordering classes\n","ordering_classes = [\"FM\", \"NM\", \"FiM\", \"AFM\"]\n","\n","# Define a function to load JSON data\n","def load_json(file_path):\n","    try:\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"✅ Loaded {len(data)} records from {file_path}\")\n","        return data\n","    except FileNotFoundError:\n","        print(f\"❌ File not found: {file_path}\")\n","        return []\n","    except json.JSONDecodeError:\n","        print(f\"❌ JSON decode error in file: {file_path}\")\n","        return []\n","    except Exception as e:\n","        print(f\"❌ Unexpected error loading {file_path}: {e}\")\n","        return []\n","\n","# Define a function to save JSON data\n","def save_json(data, file_path):\n","    try:\n","        with open(file_path, 'w') as f:\n","            json.dump(data, f, indent=4)\n","        print(f\"✅ Saved {len(data)} records to {file_path}\")\n","    except Exception as e:\n","        print(f\"❌ Error saving to {file_path}: {e}\")\n","\n","# Define a function to encode ordering\n","def encode_ordering(ordering):\n","    if ordering in ordering_classes:\n","        encoding = [1 if ordering == cls else 0 for cls in ordering_classes]\n","    else:\n","        encoding = [0] * len(ordering_classes)\n","    return encoding\n","\n","# Function to process a single JSON file\n","def process_json_file(source_file, dict_filename):\n","    source_path = os.path.join(PHASE_2_DIR, source_file)\n","    dict_path = os.path.join(DICTIONARIES_DIR, dict_filename)\n","\n","    # Load data\n","    data = load_json(source_path)\n","    if not data:\n","        print(f\"❌ Skipping processing for {source_file} due to loading issues.\")\n","        return\n","\n","    # Create dictionary with unique keys\n","    formula_dict = {}\n","    formula_count = {}\n","\n","    for record in tqdm(data, desc=f\"Processing {source_file}\"):\n","        formula = record.get('formula_pretty', 'Unknown')\n","        composition_encoded = record.get('composition_encoded', [])\n","        density = record.get('density', 0)\n","        ordering_encoded = record.get('ordering_encoded', [0]*len(ordering_classes))\n","\n","        # Combine composition_encoded and density\n","        feature_vector = composition_encoded + [density]\n","\n","        # Prepare the value\n","        value = [feature_vector, ordering_encoded]\n","\n","        # Handle duplicate keys\n","        if formula in formula_dict:\n","            formula_count[formula] += 1\n","            new_key = f\"{formula}_{formula_count[formula]}\"\n","        else:\n","            formula_count[formula] = 1\n","            new_key = formula\n","\n","        formula_dict[new_key] = value\n","\n","    # Save the dictionary\n","    save_json(formula_dict, dict_path)\n","\n","# Process all source files to create dictionaries\n","for source_file, dict_filename in source_files.items():\n","    process_json_file(source_file, dict_filename)\n","\n","# Function to convert dictionaries to NumPy arrays\n","def convert_dict_to_numpy(dict_file, X_filename, Y_filename):\n","    dict_path = os.path.join(DICTIONARIES_DIR, dict_file)\n","    X_path = os.path.join(DATA_TENSORFLOW_DIR, X_filename)\n","    Y_path = os.path.join(DATA_TENSORFLOW_DIR, Y_filename)\n","\n","    # Load the dictionary\n","    formula_dict = load_json(dict_path)\n","    if not formula_dict:\n","        print(f\"❌ Skipping conversion for {dict_file} due to loading issues.\")\n","        return\n","\n","    # Initialize lists\n","    X_list = []\n","    Y_list = []\n","\n","    # Iterate through the dictionary\n","    for key, value in formula_dict.items():\n","        feature_vector, ordering_encoded = value\n","        X_list.append(feature_vector)\n","        Y_list.append(ordering_encoded)\n","\n","    # Convert to NumPy arrays\n","    X_array = np.array(X_list, dtype=np.float32)\n","    Y_array = np.array(Y_list, dtype=np.float32)\n","\n","    # Save the arrays\n","    try:\n","        np.save(X_path, X_array)\n","        print(f\"✅ Saved feature array to {X_path}\")\n","    except Exception as e:\n","        print(f\"❌ Error saving feature array to {X_path}: {e}\")\n","\n","    try:\n","        np.save(Y_path, Y_array)\n","        print(f\"✅ Saved label array to {Y_path}\")\n","    except Exception as e:\n","        print(f\"❌ Error saving label array to {Y_path}: {e}\")\n","\n","# Define mapping from source dictionary files to NumPy filenames\n","numpy_files_mapping = {\n","    'dev_f2_p2_dict.json': ('X_dev_f2_p2.npy', 'Y_dev_f2_p2.npy'),\n","    'test_f2_p2_dict.json': ('X_test_f2_p2.npy', 'Y_test_f2_p2.npy'),\n","    'training_f2_p2_dict.json': ('X_training_f2_p2.npy', 'Y_training_f2_p2.npy'),\n","    'training_dev_f2_p2_dict.json': ('X_training_dev_f2_p2.npy', 'Y_training_dev_f2_p2.npy')\n","}\n","\n","# Convert all dictionaries to NumPy arrays\n","for dict_file, (X_filename, Y_filename) in numpy_files_mapping.items():\n","    convert_dict_to_numpy(dict_file, X_filename, Y_filename)\n","\n","print(\"\\n🎉 All processing complete! The 'Phase 3 Data' folder now contains the 'dictionaries' and 'Data_Tensorflow' subfolders with the respective files.\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"s8yYliqQXqYI","executionInfo":{"status":"ok","timestamp":1734022493935,"user_tz":-60,"elapsed":7,"user":{"displayName":"Muhamad Kussai Alawad","userId":"07092718358044543588"}}},"execution_count":4,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1g38fgpXe2Igw2jhYbzdmziFWDhOz-R1z","authorship_tag":"ABX9TyNk4r2c/LYk6yjEpYa/ou00"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
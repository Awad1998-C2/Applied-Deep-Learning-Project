{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1fAdKZEumqRNIKoz9hMAJXW241z2CV6_h","authorship_tag":"ABX9TyMC6FdUnA4yvKAiX0EQSpLw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"oAayTmSBUmEX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734547664689,"user_tz":-60,"elapsed":139919,"user":{"displayName":"Muhamad Kussai Alawad","userId":"07092718358044543588"}},"outputId":"9ae998e6-e2cb-4757-9859-4403929cdb40"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✅ Target directory already exists: /content/drive/MyDrive/Colab Notebooks/Datasets/additional_datasets\n","\n","🔄 Processing file: /content/drive/MyDrive/Colab Notebooks/Datasets/RawData/labeled_empirical_data_f2_p1.json\n","✅ Loaded 48649 records from /content/drive/MyDrive/Colab Notebooks/Datasets/RawData/labeled_empirical_data_f2_p1.json\n"]},{"output_type":"stream","name":"stderr","text":["Processing /content/drive/MyDrive/Colab Notebooks/Datasets/RawData/labeled_empirical_data_f2_p1.json: 100%|██████████| 48649/48649 [00:01<00:00, 29620.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Saved 48649 records to /content/drive/MyDrive/Colab Notebooks/Datasets/additional_datasets/labeled_empirical_data_f2_p2.json\n","\n","🎉 One-Hot Encoding and Data Transformation Complete!\n"]}],"source":["# Install required libraries\n","!pip install tqdm\n","\n","# Import necessary libraries\n","import os\n","import json\n","import random\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","element_symbols = [\n","    \"H\", \"He\",\n","    \"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"F\", \"Ne\",\n","    \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\", \"Cl\", \"Ar\",\n","    \"K\", \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\",\n","    \"Cu\", \"Zn\", \"Ga\", \"Ge\", \"As\", \"Se\", \"Br\", \"Kr\",\n","    \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\",\n","    \"Ag\", \"Cd\", \"In\", \"Sn\", \"Sb\", \"Te\", \"I\", \"Xe\",\n","    \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\",\n","    \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\", \"Yb\", \"Lu\",\n","    \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Hg\",\n","    \"Tl\", \"Pb\", \"Bi\", \"Po\", \"At\", \"Rn\",\n","    \"Fr\", \"Ra\", \"Ac\", \"Th\", \"Pa\", \"U\", \"Np\", \"Pu\",\n","    \"Am\", \"Cm\", \"Bk\", \"Cf\", \"Es\", \"Fm\", \"Md\", \"No\",\n","    \"Lr\", \"Rf\", \"Db\", \"Sg\", \"Bh\", \"Hs\", \"Mt\", \"Ds\",\n","    \"Rg\", \"Cn\", \"Nh\", \"Fl\", \"Mc\", \"Lv\", \"Ts\", \"Og\"\n","]\n","\n","# Define ordering classes and their one-hot encoding\n","ordering_classes = [\"FM\", \"NM\", \"FiM\", \"AFM\"]\n","ordering_one_hot = {\n","    \"FM\": [1, 0, 0, 0],\n","    \"NM\": [0, 1, 0, 0],\n","    \"FiM\": [0, 0, 1, 0],\n","    \"AFM\": [0, 0, 0, 1]\n","}\n","\n","# Define source and target paths\n","SOURCE_FILE = '/content/drive/MyDrive/Colab Notebooks/Datasets/RawData/labeled_empirical_data_f2_p1.json'\n","TARGET_DIR = '/content/drive/MyDrive/Colab Notebooks/Datasets/additional_datasets'\n","TARGET_FILE = 'labeled_empirical_data_f2_p2.json'\n","TARGET_PATH = os.path.join(TARGET_DIR, TARGET_FILE)\n","\n","# Create the target directory if it doesn't exist\n","if not os.path.exists(TARGET_DIR):\n","    os.makedirs(TARGET_DIR)\n","    print(f\"✅ Created target directory: {TARGET_DIR}\")\n","else:\n","    print(f\"✅ Target directory already exists: {TARGET_DIR}\")\n","\n","# Define a function to load JSON data\n","def load_json(file_path):\n","    try:\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"✅ Loaded {len(data)} records from {file_path}\")\n","        return data\n","    except FileNotFoundError:\n","        print(f\"❌ File not found: {file_path}\")\n","        return []\n","    except json.JSONDecodeError:\n","        print(f\"❌ JSON decode error in file: {file_path}\")\n","        return []\n","    except Exception as e:\n","        print(f\"❌ Unexpected error loading {file_path}: {e}\")\n","        return []\n","\n","# Define a function to save JSON data\n","def save_json(data, file_path):\n","    try:\n","        with open(file_path, 'w') as f:\n","            json.dump(data, f, indent=4)\n","        print(f\"✅ Saved {len(data)} records to {file_path}\")\n","    except Exception as e:\n","        print(f\"❌ Error saving to {file_path}: {e}\")\n","\n","def encode_composition(composition_reduced):\n","    composition_encoded = []\n","    for element in element_symbols:\n","        count = composition_reduced.get(element, 0)\n","        # If count is not an integer or float, treat it as 0.\n","        if not isinstance(count, (int, float)):\n","            count = 0\n","        composition_encoded.append(count)\n","\n","    # Assert that we have exactly 118 elements after encoding.\n","    assert len(composition_encoded) == 118, (\n","        f\"composition_encoded length is {len(composition_encoded)}, \"\n","        f\"expected 118. Please verify your element_symbols list and data processing.\"\n","    )\n","    return composition_encoded\n","\n","def encode_ordering(ordering):\n","    encoded = ordering_one_hot.get(ordering, [0, 0, 0, 0])\n","    if encoded == [0, 0, 0, 0] and ordering not in ordering_one_hot:\n","        print(f\"⚠️ Unrecognized ordering class: {ordering}. Encoding as all zeros.\")\n","    return encoded\n","\n","# Process the single source file\n","print(f\"\\n🔄 Processing file: {SOURCE_FILE}\")\n","data = load_json(SOURCE_FILE)\n","\n","if not data:\n","    print(f\"❌ No data loaded from {SOURCE_FILE}, cannot process.\")\n","else:\n","    processed_data = []\n","    for record in tqdm(data, desc=f\"Processing {SOURCE_FILE}\"):\n","        # Extract required fields\n","        formula_pretty = record.get('formula_pretty', \"\")\n","        composition_reduced = record.get('composition_reduced', {})\n","        density = record.get('density', None)\n","        ordering = record.get('ordering', \"\")\n","\n","        # Encode composition\n","        composition_encoded = encode_composition(composition_reduced)\n","\n","        # Encode ordering\n","        ordering_encoded = encode_ordering(ordering)\n","\n","        new_record = {\n","            \"formula_pretty\": formula_pretty,\n","            \"composition_reduced\": composition_reduced,\n","            \"composition_encoded\": composition_encoded,\n","            \"density\": density,\n","            \"ordering\": ordering,\n","            \"ordering_encoded\": ordering_encoded\n","        }\n","\n","        processed_data.append(new_record)\n","\n","    # Save the processed data to the target file\n","    save_json(processed_data, TARGET_PATH)\n","\n","print(\"\\n🎉 One-Hot Encoding and Data Transformation Complete!\")\n"]},{"cell_type":"code","source":["# Install required libraries\n","!pip install tqdm\n","\n","# Import necessary libraries\n","import os\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","element_symbols = [\n","    \"H\", \"He\",\n","    \"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"F\", \"Ne\",\n","    \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\", \"Cl\", \"Ar\",\n","    \"K\", \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\",\n","    \"Cu\", \"Zn\", \"Ga\", \"Ge\", \"As\", \"Se\", \"Br\", \"Kr\",\n","    \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\",\n","    \"Ag\", \"Cd\", \"In\", \"Sn\", \"Sb\", \"Te\", \"I\", \"Xe\",\n","    \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\",\n","    \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\", \"Yb\", \"Lu\",\n","    \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Hg\",\n","    \"Tl\", \"Pb\", \"Bi\", \"Po\", \"At\", \"Rn\",\n","    \"Fr\", \"Ra\", \"Ac\", \"Th\", \"Pa\", \"U\", \"Np\", \"Pu\",\n","    \"Am\", \"Cm\", \"Bk\", \"Cf\", \"Es\", \"Fm\", \"Md\", \"No\",\n","    \"Lr\", \"Rf\", \"Db\", \"Sg\", \"Bh\", \"Hs\", \"Mt\", \"Ds\",\n","    \"Rg\", \"Cn\", \"Nh\", \"Fl\", \"Mc\", \"Lv\", \"Ts\", \"Og\"\n","]\n","\n","# Define ordering classes and their one-hot encoding\n","ordering_one_hot = {\n","    \"FM\": [1, 0, 0, 0],\n","    \"NM\": [0, 1, 0, 0],\n","    \"FiM\": [0, 0, 1, 0],\n","    \"AFM\": [0, 0, 0, 1]\n","}\n","\n","# Input and output paths\n","SOURCE_FILE = '/content/drive/MyDrive/Colab Notebooks/Datasets/RawData/labeled_empirical_data_f2_p1.json'\n","PHASE_3_DIR = '/content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data'\n","\n","# Create the phase 3 directory if it doesn't exist\n","if not os.path.exists(PHASE_3_DIR):\n","    os.makedirs(PHASE_3_DIR)\n","    print(f\"✅ Created directory: {PHASE_3_DIR}\")\n","else:\n","    print(f\"✅ Directory already exists: {PHASE_3_DIR}\")\n","\n","X_OUTPUT_FILE = os.path.join(PHASE_3_DIR, 'X_train_empirical.npy')\n","Y_OUTPUT_FILE = os.path.join(PHASE_3_DIR, 'Y_train_empirical.npy')\n","\n","# Define a function to load JSON data\n","def load_json(file_path):\n","    try:\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"✅ Loaded {len(data)} records from {file_path}\")\n","        return data\n","    except FileNotFoundError:\n","        print(f\"❌ File not found: {file_path}\")\n","        return []\n","    except json.JSONDecodeError:\n","        print(f\"❌ JSON decode error in file: {file_path}\")\n","        return []\n","    except Exception as e:\n","        print(f\"❌ Unexpected error loading {file_path}: {e}\")\n","        return []\n","\n","def encode_composition(composition_reduced):\n","    composition_encoded = []\n","    for element in element_symbols:\n","        count = composition_reduced.get(element, 0)\n","        # If count is not an integer or float, set to 0\n","        if not isinstance(count, (int, float)):\n","            count = 0\n","        composition_encoded.append(count)\n","\n","    # Verify length is correct\n","    assert len(composition_encoded) == 118, (\n","        f\"composition_encoded length is {len(composition_encoded)}, expected 118.\"\n","    )\n","    return composition_encoded\n","\n","def encode_ordering(ordering):\n","    encoded = ordering_one_hot.get(ordering, [0, 0, 0, 0])\n","    return encoded\n","\n","# Load the data\n","data = load_json(SOURCE_FILE)\n","\n","if not data:\n","    print(f\"❌ No data loaded from {SOURCE_FILE}, cannot process.\")\n","else:\n","    X_list = []\n","    Y_list = []\n","\n","    for record in tqdm(data, desc=f\"Processing {SOURCE_FILE}\"):\n","        composition_reduced = record.get('composition_reduced', {})\n","        ordering = record.get('ordering', \"\")\n","        density = record.get('density', 0.0)  # Default to 0.0 if 'density' is missing\n","\n","        # Encode composition and ordering\n","        composition_encoded = encode_composition(composition_reduced)\n","        ordering_encoded = encode_ordering(ordering)\n","\n","        # Validate and append density\n","        if isinstance(density, (int, float)):\n","            composition_encoded.append(density)\n","        else:\n","            # Handle invalid density values by setting to 0.0 or another appropriate default\n","            composition_encoded.append(0.0)\n","\n","        # Now, composition_encoded has 119 elements\n","        assert len(composition_encoded) == 119, (\n","            f\"Feature vector length is {len(composition_encoded)}, expected 119.\"\n","        )\n","\n","        # Append to lists\n","        X_list.append(composition_encoded)\n","        Y_list.append(ordering_encoded)\n","\n","    # Convert to NumPy arrays\n","    X_array = np.array(X_list, dtype=np.float32)\n","    Y_array = np.array(Y_list, dtype=np.float32)\n","\n","    # Save the arrays\n","    np.save(X_OUTPUT_FILE, X_array)\n","    np.save(Y_OUTPUT_FILE, Y_array)\n","\n","    print(f\"✅ Saved X to {X_OUTPUT_FILE} with shape {X_array.shape}\")\n","    print(f\"✅ Saved Y to {Y_OUTPUT_FILE} with shape {Y_array.shape}\")\n","    print(\"\\n🎉 Conversion to NumPy arrays and saving complete!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kiMZMlaQXBOt","executionInfo":{"status":"ok","timestamp":1734547676880,"user_tz":-60,"elapsed":12208,"user":{"displayName":"Muhamad Kussai Alawad","userId":"07092718358044543588"}},"outputId":"f2919347-86c9-4585-b9b5-97280d824c82"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✅ Directory already exists: /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data\n","✅ Loaded 48649 records from /content/drive/MyDrive/Colab Notebooks/Datasets/RawData/labeled_empirical_data_f2_p1.json\n"]},{"output_type":"stream","name":"stderr","text":["Processing /content/drive/MyDrive/Colab Notebooks/Datasets/RawData/labeled_empirical_data_f2_p1.json: 100%|██████████| 48649/48649 [00:03<00:00, 13210.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Saved X to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/X_train_empirical.npy with shape (48649, 119)\n","✅ Saved Y to /content/drive/MyDrive/Colab Notebooks/Datasets/Phase 3 Data/Y_train_empirical.npy with shape (48649, 4)\n","\n","🎉 Conversion to NumPy arrays and saving complete!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"uM_1uq1VmzpY","executionInfo":{"status":"ok","timestamp":1734547676881,"user_tz":-60,"elapsed":11,"user":{"displayName":"Muhamad Kussai Alawad","userId":"07092718358044543588"}}},"execution_count":2,"outputs":[]}]}